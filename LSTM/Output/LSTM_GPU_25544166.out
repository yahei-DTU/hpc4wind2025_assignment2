Using device: cuda
shape of energy dataset: (35064, 29)
shape of weather features: (178396, 17)
preprocessing data...
vanilla_lstm
stacked_lstm
model_bilstm
{'vanilla_lstm': LSTMModel(
  (lstm): LSTM(30, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
), 'stacked_lstm': LSTMModel(
  (lstm): LSTM(30, 32, num_layers=2, batch_first=True, dropout=0.2)
  (fc): Linear(in_features=32, out_features=1, bias=True)
), 'model_bilstm': LSTMModel(
  (lstm): LSTM(30, 32, num_layers=5, batch_first=True, bidirectional=True)
  (fc): Linear(in_features=64, out_features=1, bias=True)
)}
===== training vanilla_lstm =====
[1/500] Training loss: 1.0288
[2/500] Training loss: 1.0179
[3/500] Training loss: 1.0085
[4/500] Training loss: 0.9997
[5/500] Training loss: 0.9914
[6/500] Training loss: 0.9837
[7/500] Training loss: 0.9764
[8/500] Training loss: 0.9695
[9/500] Training loss: 0.9629
[10/500] Training loss: 0.9567
[50/500] Training loss: 0.8129
[100/500] Training loss: 0.6988
[150/500] Training loss: 0.5953
[200/500] Training loss: 0.5012
[250/500] Training loss: 0.4184
[300/500] Training loss: 0.3462
[350/500] Training loss: 0.2851
[400/500] Training loss: 0.2369
[450/500] Training loss: 0.2000
[500/500] Training loss: 0.1714
Training time for vanilla_lstm: 50.1104 seconds
Mean time per epoch for vanilla_lstm: 0.1002 seconds
model_path ./models/vanilla_lstm.pth
==== plot losses - vanilla_lstm ====== 
===== scores for vanilla_lstm ====
vanilla_lstm: [0.355] 1.1, 1.5, 1.0, 1.3, 1.2, 1.4, 1.2
===== training stacked_lstm =====
[1/500] Training loss: 1.0220
[2/500] Training loss: 1.0159
[3/500] Training loss: 1.0103
[4/500] Training loss: 1.0060
[5/500] Training loss: 1.0012
[6/500] Training loss: 0.9965
[7/500] Training loss: 0.9918
[8/500] Training loss: 0.9879
[9/500] Training loss: 0.9831
[10/500] Training loss: 0.9790
[50/500] Training loss: 0.7950
[100/500] Training loss: 0.6886
[150/500] Training loss: 0.6035
[200/500] Training loss: 0.5350
[250/500] Training loss: 0.4800
[300/500] Training loss: 0.4349
[350/500] Training loss: 0.3836
[400/500] Training loss: 0.3394
[450/500] Training loss: 0.3159
[500/500] Training loss: 0.2877
Training time for stacked_lstm: 57.5650 seconds
Mean time per epoch for stacked_lstm: 0.1151 seconds
model_path ./models/stacked_lstm.pth
==== plot losses - stacked_lstm ====== 
===== scores for stacked_lstm ====
stacked_lstm: [0.259] 1.1, 1.4, 0.9, 1.3, 1.1, 1.1, 1.1
===== training model_bilstm =====
[1/500] Training loss: 1.0032
[2/500] Training loss: 1.0011
[3/500] Training loss: 0.9999
[4/500] Training loss: 0.9989
[5/500] Training loss: 0.9979
[6/500] Training loss: 0.9964
[7/500] Training loss: 0.9937
[8/500] Training loss: 0.9872
[9/500] Training loss: 0.9700
[10/500] Training loss: 0.9374
[50/500] Training loss: 0.7055
[100/500] Training loss: 0.5269
[150/500] Training loss: 0.3554
[200/500] Training loss: 0.2285
[250/500] Training loss: 0.1465
[300/500] Training loss: 0.0827
[350/500] Training loss: 0.0548
[400/500] Training loss: 0.0397
[450/500] Training loss: 0.0303
[500/500] Training loss: 0.0251
Training time for model_bilstm: 132.9984 seconds
Mean time per epoch for model_bilstm: 0.2660 seconds
model_path ./models/model_bilstm.pth
==== plot losses - model_bilstm ====== 
===== scores for model_bilstm ====
model_bilstm: [0.303] 1.2, 1.4, 1.2, 1.3, 1.3, 1.1, 1.1
RMSE for Vanilla LSTM:  11800793.930732014
RMSE for BILSTM:  11880857.599986736

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 25544166: <LSTM_GPU> in cluster <dcc> Done

Job <LSTM_GPU> was submitted from host <n-62-27-19> by user <yahei> in cluster <dcc> at Thu Jul 10 13:12:57 2025
Job was executed on host(s) <4*n-62-20-12>, in queue <gpuv100>, as user <yahei> in cluster <dcc> at Thu Jul 10 13:12:57 2025
</zhome/25/9/211757> was used as the home directory.
</zhome/25/9/211757/hpc4wind2025_assignment2/LSTM> was used as the working directory.
Started at Thu Jul 10 13:12:57 2025
Terminated at Thu Jul 10 13:17:17 2025
Results reported at Thu Jul 10 13:17:17 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
#BSUB -q gpuv100
#BSUB -J LSTM_GPU
#BSUB -P hpc4wind2025
#BSUB -n 4
#BSUB -u yahei@dtu.dk
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=1GB]"
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -W 00:30
#BSUB -R "select[gpu32gb]"
#BSUB -o Output/LSTM_GPU_%J.out
#BSUB -e Output/LSTM_GPU_%J.err

source /dtu/projects/HPC4Wind_2025/conda/conda_init.sh
conda activate hpc4wind

#script here
python /zhome/25/9/211757/hpc4wind2025_assignment2/LSTM/LSTM.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   248.00 sec.
    Max Memory :                                 807 MB
    Average Memory :                             706.00 MB
    Total Requested Memory :                     4096.00 MB
    Delta Memory :                               3289.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   281 sec.
    Turnaround time :                            260 sec.

The output (if any) is above this job summary.



PS:

Read file <Output/LSTM_GPU_25544166.err> for stderr output of this job.

